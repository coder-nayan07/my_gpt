# my_gpt: Character-Level Language Models

This repository implements two character-level language models trained on the **Tiny Shakespeare** dataset. The project demonstrates the evolution from a simple probabilistic baseline to a complex Transformer-based architecture (similar to GPT) capable of generating coherent Shakespearean-style text.

## üìÇ Files

*   `bigram_model.py`: A simple baseline model that predicts the next character based solely on the current character using a lookup table.
*   `mini_gpt.py`: A robust Transformer (Decoder-only) model implementing Self-Attention, Multi-Head Attention, Feed-Forward Networks, and Residual connections.
*   `input.txt`: The training dataset (Tiny Shakespeare).
*   `output.txt`: Sample text generated by the Transformer model.
*   `output_bigram.txt`: Sample text generated by the Bigram model.

## üöÄ Usage

To train the models and generate text, simply run the Python scripts. The scripts are set to auto-detect CUDA if available.

### 1. Run the Baseline (Bigram)
```bash
python bigram_model.py
```

### 2. Run the Transformer (Mini GPT)
```bash
python mini_gpt.py
```

*Note: The `input.txt` file is required. If not present, download it from [here](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).*

## üß† Model Architectures

### Bigram Model (`bigram_model.py`)
*   **Architecture:** Simple Embedding Lookup (`nn.Embedding`).
*   **Mechanism:** Predicts the next token logits based only on the immediate identity of the current token.
*   **Context:** 1 character.
*   **Performance:** High loss, generates incoherent character soup.

### Mini GPT (`mini_gpt.py`)
*   **Architecture:** Decoder-only Transformer.
*   **Features:**
    *   Positional Embeddings
    *   Multi-Head Self-Attention (with masking)
    *   Feed-Forward Neural Networks
    *   Residual Connections & Layer Normalization
    *   Dropout regularization
*   **Parameter Count:** ~10.7 Million parameters.

## ‚öôÔ∏è Hyperparameters (Mini GPT)

The Transformer model is configured with the following parameters:

| Parameter | Value |
| :--- | :--- |
| **Batch Size** | 64 |
| **Block Size (Context)** | 256 |
| **Embedding Dim** | 384 |
| **Heads** | 6 |
| **Layers** | 6 |
| **Dropout** | 0.2 |
| **Learning Rate** | 3e-4 |
| **Max Iterations** | 5000 |

## üìä Results Comparison

The difference in generation quality demonstrates the power of the Transformer architecture in capturing long-range dependencies.

### Bigram Output (Baseline)
*The output is mostly nonsensical and fails to form real words.*
```text
od nos CAy go ghanoray t, co haringoudrounclethe k, leve fr werar,
Is fa!
Thilemer ciat p mboomyorarifrcitheve m...
```

### Mini GPT Output (Transformer)
*The output captures English syntax, Shakespearean vocabulary, and dramatic structure (characters, dialogue).*
```text
DUKE VINCENTIO:
Not as that appeals hind our sightstain blamy
Dishop gives upon the world.

LUCIO:
He blessed what?

Nurse:
I am direct:
If, the middle is so.

LUCIO:
No, and tell me we there needs to't:
My lord great will you no remedy.
```

## üìö Acknowledgements
This project is inspired by Andrej Karpathy's "Zero to Hero" series on building GPT from scratch.
```
